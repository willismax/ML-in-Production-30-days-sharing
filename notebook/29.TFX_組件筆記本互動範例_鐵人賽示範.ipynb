{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "TFX 組件筆記本互動範例-鐵人幫示範.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/willismax/ML-in-Production-30-days-sharing/blob/main/notebook/29.TFX_%E7%B5%84%E4%BB%B6%E7%AD%86%E8%A8%98%E6%9C%AC%E4%BA%92%E5%8B%95%E7%AF%84%E4%BE%8B_%E9%90%B5%E4%BA%BA%E8%B3%BD%E7%A4%BA%E7%AF%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23R0Z9RojXYW"
      },
      "source": [
        "# TFX 組件筆記本互動實作\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ryYMp2D8uDO"
      },
      "source": [
        "- 此為鐵人幫範例，內容源自[TFX官方範例](https://www.tensorflow.org/tfx/tutorials/tfx/components_keras)。\n",
        "- 本示範將使用 TensorFlow Extended (TFX) 各組件完成機械學習端對端任務，之後您也可以透過 Apache Airflow 及 Apache Beam 編排。\n",
        "- ML中繼資料 (ML Metadata) 是保存 TFX 各組件執行歷程的重要資料庫，數據可保存在 MySQL 或 SQLite 資料庫，在 Colab 示範時是暫存在 SQLite。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lZ2AObgCIqy"
      },
      "source": [
        "## 安裝與設置 TFX 環境"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as4OTe2ukSqm"
      },
      "source": [
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "  !pip install --upgrade pip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4SQA7Q5nej3"
      },
      "source": [
        "!pip install -U tfx \n",
        "\"請記得安裝完需重新啟動執行階段(Restart Runtime)，再進行後續內容\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoQc_YW9Cbg0"
      },
      "source": [
        "**安裝完需重新啟動執行階段(Restart Runtime)，再進行後續內容**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIqpWK9efviJ"
      },
      "source": [
        "import os\n",
        "import pprint\n",
        "import tempfile\n",
        "import urllib\n",
        "\n",
        "import absl\n",
        "import tensorflow as tf\n",
        "import tensorflow_model_analysis as tfma\n",
        "tf.get_logger().propagate = False\n",
        "pp = pprint.PrettyPrinter()\n",
        "\n",
        "from tfx import v1 as tfx\n",
        "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
        "\n",
        "%load_ext tfx.orchestration.experimental.interactive.notebook_extensions.skip\n",
        "\n",
        "print(f'TensorFlow version: {tf.__version__}') # >= 2.5.1\n",
        "print(f'TFX version: {tfx.__version__}') # >= 1.2.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufJKQ6OvkJlY"
      },
      "source": [
        "- 設定 Pipeline 工作路徑"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ad5JLpKbf6sN"
      },
      "source": [
        "# TFX 模組安裝原始路徑_tfx_root\n",
        "_tfx_root = tfx.__path__[0]\n",
        "\n",
        "# 芝加哥計程車資料集路徑_taxi_root\n",
        "_taxi_root = os.path.join(_tfx_root, 'examples/chicago_taxi_pipeline')\n",
        "\n",
        "# 模型發布serving路徑_serving_model_dir\n",
        "_serving_model_dir = os.path.join(tempfile.mkdtemp(), 'serving_model/taxi_simple')\n",
        "\n",
        "# Set up logging.\n",
        "absl.logging.set_verbosity(absl.logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2cMMAbSkGfX"
      },
      "source": [
        "- 下載資料集，以芝加哥[Taxi Trips dataset](https://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew) 進行示範，特徵如下，將使用這個數據集構建一個預測小費`tips`的模型。\n",
        "\n",
        "<table>\n",
        "<tr><td>pickup_community_area</td><td>fare</td><td>trip_start_month</td></tr>\n",
        "<tr><td>trip_start_hour</td><td>trip_start_day</td><td>trip_start_timestamp</td></tr>\n",
        "<tr><td>pickup_latitude</td><td>pickup_longitude</td><td>dropoff_latitude</td></tr>\n",
        "<tr><td>dropoff_longitude</td><td>trip_miles</td><td>pickup_census_tract</td></tr>\n",
        "<tr><td>dropoff_census_tract</td><td>payment_type</td><td>company</td></tr>\n",
        "<tr><td>trip_seconds</td><td>dropoff_community_area</td><td>tips</td></tr>\n",
        "</table>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BywX6OUEhAqn"
      },
      "source": [
        "# 建立/tmp/tfx-dataXXXXXXZ/檔案路徑_data_filepath\n",
        "_data_root = tempfile.mkdtemp(prefix='tfx-data')\n",
        "DATA_PATH = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/data/simple/data.csv'\n",
        "_data_filepath = os.path.join(_data_root, \"data.csv\")\n",
        "urllib.request.urlretrieve(DATA_PATH, _data_filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5YPeLPFOXaD"
      },
      "source": [
        "# 查看資料集檔案確認內容\n",
        "!head {_data_filepath}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ONIE_hdkPS4"
      },
      "source": [
        "### 創建 InteractiveContext\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59XJ9LmPHZLx"
      },
      "source": [
        "- `tfx.orchestration.experimental.interactive.interactive_context.InteractiveContext` 允許在 notebook 環境中以互動方式查看 TFX 組件。\n",
        "- `InteractiveContext` 預設使用臨時的中繼資料。\n",
        "  - 已有自己的 pipeline 可設定 `pipe_root` 參數。\n",
        "  - 已有中繼資料庫可設定 `metadata_connection_config` 參數。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Rh6K5sUf9dd"
      },
      "source": [
        "context = InteractiveContext()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdQWxfsVkzdJ"
      },
      "source": [
        "## 互動式 TFX components 示範"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gH7_JAzIF5R"
      },
      "source": [
        "- 本範例將逐一示範各組建的工作，也透過 `InteractiveContext()` 逐一演示互動情形。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "946mGO_GVFkL"
      },
      "source": [
        "### ExampleGen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yisZN7fVJIh"
      },
      "source": [
        "1.  將數據拆分為訓練集和評估集（默認情況下，2/3 訓練 + 1/3 評估）\n",
        "2.  將數據轉換為 `tf.Example` 格式（參閱[說明](https://www.tensorflow.org/tutorials/load_data/tfrecord)）。\n",
        "3.  將數據複製到 `_tfx_root` 目錄中供其他組件訪問。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDJj2yBR6d5X"
      },
      "source": [
        "- 本範例將 `_data_root` 的 CSV 資料集輸入至 `ExampleGen`。\n",
        "\n",
        "- 注意：在這個 notebook 示範使用`InteractiveContext.run()`。在生產環境中，會預指定所有組件`Pipeline`以傳遞給協調器（請參閱[構建 TFX 管道指南](https://www.tensorflow.org/tfx/guide/build_tfx_pipeline)）。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyXjuMt8f-9u"
      },
      "source": [
        "example_gen = tfx.components.CsvExampleGen(input_base=_data_root)\n",
        "context.run(example_gen, enable_cache=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqCoZh7KPUm9"
      },
      "source": [
        "- `ExampleGen` 組件將輸出`training examples` 、 `evaluation examples` 。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "880KkTAkPeUg"
      },
      "source": [
        "artifact = example_gen.outputs['examples'].get()[0]\n",
        "print(artifact.split_names, artifact.uri)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6vcbW_wPqvl"
      },
      "source": [
        "- 輸出前3筆資料觀察"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4XIXjiCPwzQ"
      },
      "source": [
        "# Get the URI of the output artifact representing the training examples, which is a directory\n",
        "train_uri = os.path.join(\n",
        "    example_gen.outputs['examples'].get()[0].uri,\n",
        "    'Split-train'\n",
        "    )\n",
        "\n",
        "# Get the list of files in this directory (all compressed TFRecord files)\n",
        "tfrecord_filenames = [\n",
        "    os.path.join(train_uri, name) \n",
        "    for name in os.listdir(train_uri)\n",
        "    ]\n",
        "\n",
        "# Create a `TFRecordDataset` to read these files\n",
        "dataset = tf.data.TFRecordDataset(\n",
        "    tfrecord_filenames, \n",
        "    compression_type=\"GZIP\"\n",
        "    )\n",
        "\n",
        "# Iterate over the first 3 records and decode them.\n",
        "for tfrecord in dataset.take(3):\n",
        "  serialized_example = tfrecord.numpy()\n",
        "  example = tf.train.Example()\n",
        "  example.ParseFromString(serialized_example)\n",
        "  pp.pprint(example)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gluYjccf-IP"
      },
      "source": [
        "- `ExampleGen` 已攝取資料，接續資料分析。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTvhjuSB88xJ"
      },
      "source": [
        "### StatisticsGen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1xBbEsM9N-U"
      },
      "source": [
        "- `StatisticsGen` 組件輸入 `ExampleGen` 數據後，將據以計算出資料集的統計數據。\n",
        "- `StatisticsGen` 是 [TFDV](https://www.tensorflow.org/tfx/data_validation/get_started) 模組功能之一。\n",
        "- `context.run(statistics_gen)` 觀察互動介面，`.execution_id` 版次累加至2，`.component.inputs` 組件輸入為 `Examples` ， 輸出為 `ExampleStatistics` 。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAscCCYWgA-9"
      },
      "source": [
        "statistics_gen = tfx.components.StatisticsGen(\n",
        "    examples=example_gen.outputs['examples'])\n",
        "context.run(statistics_gen, enable_cache=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLI6cb_5WugZ"
      },
      "source": [
        "- `context.show(statistics_gen.outputs['statistics'])` 如同 TFDV 工具以  [Facets](https://pair-code.github.io/facets/) 視覺化統計資訊。\n",
        "- 可以觀察判讀可能異常的紅色值、資料分佈情形等。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLjXy7K6Tp_G"
      },
      "source": [
        "context.show(statistics_gen.outputs['statistics'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erZ2kHnUAcdR"
      },
      "source": [
        "### SchemaGen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6ZwOWD5AeiX"
      },
      "source": [
        "- `SchemaGen`組件會依據您的資料統計自動產生 Schema ，包含數據預期邊界、資料類型與屬性它還使用[TensorFlow 數據驗證](https://www.tensorflow.org/tfx/data_validation/get_started)庫。\n",
        "- `SchemaGen` 同樣是 [TFDV](https://www.tensorflow.org/tfx/data_validation/get_started) 模組功能之一。\n",
        "- 即便 Schema 自動生成已經很實用，但您仍應該會依據需求進行審查和修改。\n",
        "- `SchemaGen` 輸入為 `StatisticsGen`，默認情況下查看已拆分的訓練資料集。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygQvZ6hsiQ_J"
      },
      "source": [
        "schema_gen = tfx.components.SchemaGen(\n",
        "    statistics=statistics_gen.outputs['statistics'],\n",
        "    infer_feature_shape=False)\n",
        "context.run(schema_gen, enable_cache=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi6TxTUKXM6b"
      },
      "source": [
        "- `SchemaGen` 執行後可透過 `context.show(schema_gen.outputs['schema'])` 查看  Schema 表格。\n",
        "- 表格呈現各特徵名稱、屬性、是否必須、所有值、Domain 及 邊界範圍等，\n",
        "參見 [SchemaGen 文件](https://www.tensorflow.org/tfx/guide/schemagen).。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec9vqDXpXeMb"
      },
      "source": [
        "context.show(schema_gen.outputs['schema'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtCXfNpnDtCI"
      },
      "source": [
        "### ExampleValidator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJmdSf9GDqc9"
      },
      "source": [
        "- `ExampleValidator` 組件根據 Schema 的預期檢測數據中的異常。\n",
        "- `ExampleValidator` 同樣是 [TFDV](https://www.tensorflow.org/tfx/data_validation/get_started) 模組功能之一。\n",
        "- `ExampleValidator` 的輸入是來自具有數據統計資訊的 `StatisticsGen` 以及具有數據定義 Schema 的 `SchemaGen`。\n",
        "- `ExampleValidator` 的輸出 `anomalies` 是有無異常的判讀結果。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRlRUuGgiXks"
      },
      "source": [
        "example_validator = tfx.components.ExampleValidator(\n",
        "    statistics=statistics_gen.outputs['statistics'],\n",
        "    schema=schema_gen.outputs['schema'])\n",
        "context.run(example_validator, enable_cache=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "855mrHgJcoer"
      },
      "source": [
        "- 執行 `ExampleValidator` 後可以產生異常情形的圖表，綠字 No anomalies found. 表示無異常。\n",
        "- 由於此為最初的數據集資訊，而且統計與 Schema 皆是由該數據產生，理應無異常。未來不同版次的資訊流可能會檢測出異常情形。\n",
        "- 資料驗證可用 Schema 保護未來數據，異常偵測可用於調試模型性能、了解數據如何隨時間演變以及識別數據錯誤。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDyAAozQcrk3"
      },
      "source": [
        "context.show(example_validator.outputs['anomalies'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlM0m9LaKXUS"
      },
      "source": [
        "### Transform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVDaIT96I2gl"
      },
      "source": [
        "- `Transform` 組件為訓練和服務執行特徵工程。\n",
        "- `Transform` 使用[TensorFlow Transform](https://www.tensorflow.org/tfx/transform/get_started) 模組。\n",
        "- `Transform` 輸入數據來自 `ExampleGen` 、 Schema 來自 `SchemaGen` ，以及自行定義如何進行特徵工程的模組。\n",
        "\n",
        "- 以下為自行定義的 Transform 程式碼範例，（有關 TensorFlow Transform API 的介紹，[請參閱教程](https://www.tensorflow.org/tfx/tutorials/transform/simple)）。\n",
        "\n",
        "- Notebook 魔術指令 `%%writefile` ，可以將 cell 內的程式碼指定保存為檔案，該檔案可以用 `Transform` 組件將程式碼檔案做為模組輸入執行。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuNSiUKb4YJf"
      },
      "source": [
        "_taxi_constants_module_file = 'taxi_constants.py'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPjhXuIF4YJh"
      },
      "source": [
        "%%writefile {_taxi_constants_module_file}\n",
        "\n",
        "NUMERICAL_FEATURES = ['trip_miles', 'fare', 'trip_seconds']\n",
        "\n",
        "BUCKET_FEATURES = [\n",
        "    'pickup_latitude', 'pickup_longitude', 'dropoff_latitude',\n",
        "    'dropoff_longitude'\n",
        "]\n",
        "# tf.transform用於編碼每個特徵的桶數=10\n",
        "FEATURE_BUCKET_COUNT = 10\n",
        "\n",
        "CATEGORICAL_NUMERICAL_FEATURES = [\n",
        "    'trip_start_hour', 'trip_start_day', 'trip_start_month',\n",
        "    'pickup_census_tract', 'dropoff_census_tract', 'pickup_community_area',\n",
        "    'dropoff_community_area'\n",
        "]\n",
        "\n",
        "CATEGORICAL_STRING_FEATURES = [\n",
        "    'payment_type',\n",
        "    'company',\n",
        "]\n",
        "\n",
        "# tf.transform用於編碼VOCAB_FEATURES的詞彙術語數量=1000\n",
        "VOCAB_SIZE = 1000\n",
        "\n",
        "# Count of out-of-vocab buckets in which unrecognized \n",
        "# VOCAB_FEATURES are hashed.\n",
        "OOV_SIZE = 10\n",
        "\n",
        "# Keys\n",
        "LABEL_KEY = 'tips'\n",
        "FARE_KEY = 'fare'\n",
        "\n",
        "def t_name(key):\n",
        "  \"\"\"\n",
        "  Rename the feature keys so that they don't clash with the raw keys when\n",
        "  running the Evaluator component.\n",
        "  Args:\n",
        "    key: The original feature key\n",
        "  Returns:\n",
        "    key with '_xf' appended\n",
        "  \"\"\"\n",
        "  return key + '_xf'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Duj2Ax5z4YJl"
      },
      "source": [
        "- 接著編寫 `preprocessing_fn` 將原始數據轉換特徵。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AJ9hBs94YJm"
      },
      "source": [
        "_taxi_transform_module_file = 'taxi_transform.py'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYmxxx9A4YJn"
      },
      "source": [
        "%%writefile {_taxi_transform_module_file}\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "\n",
        "# Imported files such as taxi_constants are normally cached, so changes are\n",
        "# not honored after the first import.  Normally this is good for efficiency, but\n",
        "# during development when we may be iterating code it can be a problem. To\n",
        "# avoid this problem during development, reload the file.\n",
        "import taxi_constants\n",
        "import sys\n",
        "if 'google.colab' in sys.modules:  # Testing to see if we're doing development\n",
        "  import importlib\n",
        "  importlib.reload(taxi_constants)\n",
        "\n",
        "_NUMERICAL_FEATURES = taxi_constants.NUMERICAL_FEATURES\n",
        "_BUCKET_FEATURES = taxi_constants.BUCKET_FEATURES\n",
        "_FEATURE_BUCKET_COUNT = taxi_constants.FEATURE_BUCKET_COUNT\n",
        "_CATEGORICAL_NUMERICAL_FEATURES = taxi_constants.CATEGORICAL_NUMERICAL_FEATURES\n",
        "_CATEGORICAL_STRING_FEATURES = taxi_constants.CATEGORICAL_STRING_FEATURES\n",
        "_VOCAB_SIZE = taxi_constants.VOCAB_SIZE\n",
        "_OOV_SIZE = taxi_constants.OOV_SIZE\n",
        "_FARE_KEY = taxi_constants.FARE_KEY\n",
        "_LABEL_KEY = taxi_constants.LABEL_KEY\n",
        "\n",
        "\n",
        "def _make_one_hot(x, key):\n",
        "  \"\"\"Make a one-hot tensor to encode categorical features.\n",
        "  Args:\n",
        "    X: A dense tensor\n",
        "    key: A string key for the feature in the input\n",
        "  Returns:\n",
        "    A dense one-hot tensor as a float list\n",
        "  \"\"\"\n",
        "  integerized = tft.compute_and_apply_vocabulary(x,\n",
        "          top_k=_VOCAB_SIZE,\n",
        "          num_oov_buckets=_OOV_SIZE,\n",
        "          vocab_filename=key, name=key)\n",
        "  depth = (\n",
        "      tft.experimental.get_vocabulary_size_by_name(key) + _OOV_SIZE)\n",
        "  one_hot_encoded = tf.one_hot(\n",
        "      integerized,\n",
        "      depth=tf.cast(depth, tf.int32),\n",
        "      on_value=1.0,\n",
        "      off_value=0.0)\n",
        "  return tf.reshape(one_hot_encoded, [-1, depth])\n",
        "\n",
        "\n",
        "def _fill_in_missing(x):\n",
        "  \"\"\"Replace missing values in a SparseTensor.\n",
        "  Fills in missing values of `x` with '' or 0, and converts to a dense tensor.\n",
        "  Args:\n",
        "    x: A `SparseTensor` of rank 2.  Its dense shape should have size at most 1\n",
        "      in the second dimension.\n",
        "  Returns:\n",
        "    A rank 1 tensor where missing values of `x` have been filled in.\n",
        "  \"\"\"\n",
        "  if not isinstance(x, tf.sparse.SparseTensor):\n",
        "    return x\n",
        "\n",
        "  default_value = '' if x.dtype == tf.string else 0\n",
        "  return tf.squeeze(\n",
        "      tf.sparse.to_dense(\n",
        "          tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),\n",
        "          default_value),\n",
        "      axis=1)\n",
        "\n",
        "\n",
        "def preprocessing_fn(inputs):\n",
        "  \"\"\"tf.transform's callback function for preprocessing inputs.\n",
        "  Args:\n",
        "    inputs: map from feature keys to raw not-yet-transformed features.\n",
        "  Returns:\n",
        "    Map from string feature key to transformed feature operations.\n",
        "  \"\"\"\n",
        "  outputs = {}\n",
        "  for key in _NUMERICAL_FEATURES:\n",
        "    # If sparse make it dense, setting nan's to 0 or '', and apply zscore.\n",
        "    outputs[taxi_constants.t_name(key)] = tft.scale_to_z_score(\n",
        "        _fill_in_missing(inputs[key]), name=key)\n",
        "\n",
        "  for key in _BUCKET_FEATURES:\n",
        "    outputs[taxi_constants.t_name(key)] = tf.cast(tft.bucketize(\n",
        "            _fill_in_missing(inputs[key]), _FEATURE_BUCKET_COUNT, name=key),\n",
        "            dtype=tf.float32)\n",
        "\n",
        "  for key in _CATEGORICAL_STRING_FEATURES:\n",
        "    outputs[taxi_constants.t_name(key)] = _make_one_hot(_fill_in_missing(inputs[key]), key)\n",
        "\n",
        "  for key in _CATEGORICAL_NUMERICAL_FEATURES:\n",
        "    outputs[taxi_constants.t_name(key)] = _make_one_hot(tf.strings.strip(\n",
        "        tf.strings.as_string(_fill_in_missing(inputs[key]))), key)\n",
        "\n",
        "  # Was this passenger a big tipper?\n",
        "  taxi_fare = _fill_in_missing(inputs[_FARE_KEY])\n",
        "  tips = _fill_in_missing(inputs[_LABEL_KEY])\n",
        "  outputs[_LABEL_KEY] = tf.where(\n",
        "      tf.math.is_nan(taxi_fare),\n",
        "      tf.cast(tf.zeros_like(taxi_fare), tf.int64),\n",
        "      # Test if the tip was > 20% of the fare.\n",
        "      tf.cast(\n",
        "          tf.greater(tips, tf.multiply(taxi_fare, tf.constant(0.2))), tf.int64))\n",
        "\n",
        "  return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgbmZr3sgbWW"
      },
      "source": [
        "- 將特徵工程程式傳遞給 `Transform` 組件轉換資料。\n",
        "- `Transform`組件將產生以下兩種類型的輸出：\n",
        "  - `transform_graph` 是可以執行預處理操作的圖（此圖將包含在服務和評估模型中）。\n",
        "  - `transformed_examples` 表示預處理的訓練和評估數據。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHfhth_GiZI9"
      },
      "source": [
        "transform = tfx.components.Transform(\n",
        "    examples=example_gen.outputs['examples'],\n",
        "    schema=schema_gen.outputs['schema'],\n",
        "    module_file=os.path.abspath(_taxi_transform_module_file))\n",
        "context.run(transform, enable_cache=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SClrAaEGR1O5"
      },
      "source": [
        "transform.outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyFkBd9AR1sy"
      },
      "source": [
        "- 輸出的 `transform_graph` 同時指向包含3個子目錄的目錄。\n",
        "  - `transformed_metadata`子目錄包含預處理數據的架構。\n",
        "  - `transform_fn`子目錄包含實際的預處理圖。\n",
        "  - `metadata`子目錄包含原始數據的架構。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tRw4DneR3i7"
      },
      "source": [
        "train_uri = transform.outputs['transform_graph'].get()[0].uri\n",
        "os.listdir(train_uri)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwbW2zPKR_S4"
      },
      "source": [
        "# Get the URI of the output artifact representing the transformed examples, which is a directory\n",
        "train_uri = os.path.join(transform.outputs['transformed_examples'].get()[0].uri, 'Split-train')\n",
        "\n",
        "# Get the list of files in this directory (all compressed TFRecord files)\n",
        "tfrecord_filenames = [os.path.join(train_uri, name)\n",
        "                      for name in os.listdir(train_uri)]\n",
        "\n",
        "# Create a `TFRecordDataset` to read these files\n",
        "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
        "\n",
        "# Iterate over the first 3 records and decode them.\n",
        "for tfrecord in dataset.take(3):\n",
        "  serialized_example = tfrecord.numpy()\n",
        "  example = tf.train.Example()\n",
        "  example.ParseFromString(serialized_example)\n",
        "  pp.pprint(example)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1usz841-oChV"
      },
      "source": [
        "### Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE50W0gkoNqA"
      },
      "source": [
        "- `Trainer`組件負責訓練 TensorFlow 模型。\n",
        "-  `Trainer` 預設使用 Estimator API ，如要使用 Keras API，您需要通過在 Trainer 的構造函數中設置來指定 `custom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor)` ，參閱[Generic Trainer](https://github.com/tensorflow/community/blob/master/rfcs/20200117-tfx-generic-trainer.md) 。\n",
        "- `Trainer` 的輸入來源:\n",
        "  - 來自 `SchemaGen` 的 Schema。 \n",
        "  - 來自 `Transform` 的 graph。\n",
        "  - 訓練參數。\n",
        "  - 做為模組輸入的自定義程式碼。\n",
        "\n",
        "- 以下為用戶自定義模型代碼示範（[參見 TensorFlow Keras API 介紹](https://www.tensorflow.org/guide/keras)）。\n",
        "- 創立 `taxi_trainer.py` 之後將程式碼做為模組傳遞給 `Trainer` 組件並運行它來訓練模型。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1376oq04YJt"
      },
      "source": [
        "_taxi_trainer_module_file = 'taxi_trainer.py'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf9UuNng4YJu"
      },
      "source": [
        "%%writefile {_taxi_trainer_module_file}\n",
        "\n",
        "from typing import Dict, List, Text\n",
        "\n",
        "import os\n",
        "import glob\n",
        "from absl import logging\n",
        "\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "\n",
        "from tfx import v1 as tfx\n",
        "from tfx_bsl.public import tfxio\n",
        "from tensorflow_transform import TFTransformOutput\n",
        "\n",
        "# Imported files such as taxi_constants are normally cached, so changes are\n",
        "# not honored after the first import.  Normally this is good for efficiency, but\n",
        "# during development when we may be iterating code it can be a problem. To\n",
        "# avoid this problem during development, reload the file.\n",
        "import taxi_constants\n",
        "import sys\n",
        "if 'google.colab' in sys.modules:  # Testing to see if we're doing development\n",
        "  import importlib\n",
        "  importlib.reload(taxi_constants)\n",
        "\n",
        "_LABEL_KEY = taxi_constants.LABEL_KEY\n",
        "\n",
        "_BATCH_SIZE = 40\n",
        "\n",
        "\n",
        "def _input_fn(file_pattern: List[Text],\n",
        "              data_accessor: tfx.components.DataAccessor,\n",
        "              tf_transform_output: tft.TFTransformOutput,\n",
        "              batch_size: int = 200) -> tf.data.Dataset:\n",
        "  \"\"\"Generates features and label for tuning/training.\n",
        "\n",
        "  Args:\n",
        "    file_pattern: List of paths or patterns of input tfrecord files.\n",
        "    data_accessor: DataAccessor for converting input to RecordBatch.\n",
        "    tf_transform_output: A TFTransformOutput.\n",
        "    batch_size: representing the number of consecutive elements of returned\n",
        "      dataset to combine in a single batch\n",
        "\n",
        "  Returns:\n",
        "    A dataset that contains (features, indices) tuple where features is a\n",
        "      dictionary of Tensors, and indices is a single Tensor of label indices.\n",
        "  \"\"\"\n",
        "  return data_accessor.tf_dataset_factory(\n",
        "      file_pattern,\n",
        "      tfxio.TensorFlowDatasetOptions(\n",
        "          batch_size=batch_size, label_key=_LABEL_KEY),\n",
        "      tf_transform_output.transformed_metadata.schema)\n",
        "\n",
        "def _get_tf_examples_serving_signature(model, tf_transform_output):\n",
        "  \"\"\"Returns a serving signature that accepts `tensorflow.Example`.\"\"\"\n",
        "\n",
        "  # We need to track the layers in the model in order to save it.\n",
        "  # TODO(b/162357359): Revise once the bug is resolved.\n",
        "  model.tft_layer_inference = tf_transform_output.transform_features_layer()\n",
        "\n",
        "  @tf.function(input_signature=[\n",
        "      tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')\n",
        "  ])\n",
        "  def serve_tf_examples_fn(serialized_tf_example):\n",
        "    \"\"\"Returns the output to be used in the serving signature.\"\"\"\n",
        "    raw_feature_spec = tf_transform_output.raw_feature_spec()\n",
        "    # Remove label feature since these will not be present at serving time.\n",
        "    raw_feature_spec.pop(_LABEL_KEY)\n",
        "    raw_features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)\n",
        "    transformed_features = model.tft_layer_inference(raw_features)\n",
        "    logging.info('serve_transformed_features = %s', transformed_features)\n",
        "\n",
        "    outputs = model(transformed_features)\n",
        "    # TODO(b/154085620): Convert the predicted labels from the model using a\n",
        "    # reverse-lookup (opposite of transform.py).\n",
        "    return {'outputs': outputs}\n",
        "\n",
        "  return serve_tf_examples_fn\n",
        "\n",
        "\n",
        "def _get_transform_features_signature(model, tf_transform_output):\n",
        "  \"\"\"Returns a serving signature that applies tf.Transform to features.\"\"\"\n",
        "\n",
        "  # We need to track the layers in the model in order to save it.\n",
        "  # TODO(b/162357359): Revise once the bug is resolved.\n",
        "  model.tft_layer_eval = tf_transform_output.transform_features_layer()\n",
        "\n",
        "  @tf.function(input_signature=[\n",
        "      tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')\n",
        "  ])\n",
        "  def transform_features_fn(serialized_tf_example):\n",
        "    \"\"\"Returns the transformed_features to be fed as input to evaluator.\"\"\"\n",
        "    raw_feature_spec = tf_transform_output.raw_feature_spec()\n",
        "    raw_features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)\n",
        "    transformed_features = model.tft_layer_eval(raw_features)\n",
        "    logging.info('eval_transformed_features = %s', transformed_features)\n",
        "    return transformed_features\n",
        "\n",
        "  return transform_features_fn\n",
        "\n",
        "\n",
        "def export_serving_model(tf_transform_output, model, output_dir):\n",
        "  \"\"\"Exports a keras model for serving.\n",
        "  Args:\n",
        "    tf_transform_output: Wrapper around output of tf.Transform.\n",
        "    model: A keras model to export for serving.\n",
        "    output_dir: A directory where the model will be exported to.\n",
        "  \"\"\"\n",
        "  # The layer has to be saved to the model for keras tracking purpases.\n",
        "  model.tft_layer = tf_transform_output.transform_features_layer()\n",
        "\n",
        "  signatures = {\n",
        "      'serving_default':\n",
        "          _get_tf_examples_serving_signature(model, tf_transform_output),\n",
        "      'transform_features':\n",
        "          _get_transform_features_signature(model, tf_transform_output),\n",
        "  }\n",
        "\n",
        "  model.save(output_dir, save_format='tf', signatures=signatures)\n",
        "\n",
        "\n",
        "def _build_keras_model(tf_transform_output: TFTransformOutput\n",
        "                       ) -> tf.keras.Model:\n",
        "  \"\"\"Creates a DNN Keras model for classifying taxi data.\n",
        "\n",
        "  Args:\n",
        "    tf_transform_output: [TFTransformOutput], the outputs from Transform\n",
        "\n",
        "  Returns:\n",
        "    A keras Model.\n",
        "  \"\"\"\n",
        "  feature_spec = tf_transform_output.transformed_feature_spec().copy()\n",
        "  feature_spec.pop(_LABEL_KEY)\n",
        "\n",
        "  inputs = {}\n",
        "  for key, spec in feature_spec.items():\n",
        "    if isinstance(spec, tf.io.VarLenFeature):\n",
        "      inputs[key] = tf.keras.layers.Input(\n",
        "          shape=[None], name=key, dtype=spec.dtype, sparse=True)\n",
        "    elif isinstance(spec, tf.io.FixedLenFeature):\n",
        "      # TODO(b/208879020): Move into schema such that spec.shape is [1] and not\n",
        "      # [] for scalars.\n",
        "      inputs[key] = tf.keras.layers.Input(\n",
        "          shape=spec.shape or [1], name=key, dtype=spec.dtype)\n",
        "    else:\n",
        "      raise ValueError('Spec type is not supported: ', key, spec)\n",
        "  \n",
        "  output = tf.keras.layers.Concatenate()(tf.nest.flatten(inputs))\n",
        "  output = tf.keras.layers.Dense(100, activation='relu')(output)\n",
        "  output = tf.keras.layers.Dense(70, activation='relu')(output)\n",
        "  output = tf.keras.layers.Dense(50, activation='relu')(output)\n",
        "  output = tf.keras.layers.Dense(20, activation='relu')(output)\n",
        "  output = tf.keras.layers.Dense(1)(output)\n",
        "  return tf.keras.Model(inputs=inputs, outputs=output)\n",
        "\n",
        "\n",
        "# TFX Trainer will call this function.\n",
        "def run_fn(fn_args: tfx.components.FnArgs):\n",
        "  \"\"\"Train the model based on given args.\n",
        "\n",
        "  Args:\n",
        "    fn_args: Holds args used to train the model as name/value pairs.\n",
        "  \"\"\"\n",
        "  tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n",
        "\n",
        "  train_dataset = _input_fn(fn_args.train_files, fn_args.data_accessor, \n",
        "                            tf_transform_output, _BATCH_SIZE)\n",
        "  eval_dataset = _input_fn(fn_args.eval_files, fn_args.data_accessor, \n",
        "                           tf_transform_output, _BATCH_SIZE)\n",
        "\n",
        "  model = _build_keras_model(tf_transform_output)\n",
        "\n",
        "  model.compile(\n",
        "      loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "      optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "      metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "      log_dir=fn_args.model_run_dir, update_freq='batch')\n",
        "\n",
        "  model.fit(\n",
        "      train_dataset,\n",
        "      steps_per_epoch=fn_args.train_steps,\n",
        "      validation_data=eval_dataset,\n",
        "      validation_steps=fn_args.eval_steps,\n",
        "      callbacks=[tensorboard_callback])\n",
        "\n",
        "  # Export the model.\n",
        "  export_serving_model(tf_transform_output, model, fn_args.serving_model_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "429-vvCWibO0"
      },
      "source": [
        "trainer = tfx.components.Trainer(\n",
        "    module_file=os.path.abspath(_taxi_trainer_module_file),\n",
        "    examples=transform.outputs['transformed_examples'],\n",
        "    transform_graph=transform.outputs['transform_graph'],\n",
        "    schema=schema_gen.outputs['schema'],\n",
        "    train_args=tfx.proto.TrainArgs(num_steps=10000),\n",
        "    eval_args=tfx.proto.EvalArgs(num_steps=5000))\n",
        "context.run(trainer, enable_cache=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-iP-Mu5sSSw"
      },
      "source": [
        "### 使用 TensorBoard 分析訓練"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-kLWYj7sbId"
      },
      "source": [
        "- 檢視 'Format-Serving' 目錄。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXe62WE0S0Ek"
      },
      "source": [
        "model_artifact_dir = trainer.outputs['model'].get()[0].uri\n",
        "pp.pprint(os.listdir(model_artifact_dir))\n",
        "model_dir = os.path.join(model_artifact_dir, 'Format-Serving')\n",
        "pp.pprint(os.listdir(model_dir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfjOmSro6Q3Y"
      },
      "source": [
        "- 可以透過 TensorBoard 分析模型訓練曲線。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-APzqz2NeAyj"
      },
      "source": [
        "model_run_artifact_dir = trainer.outputs['model_run'].get()[0].uri\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {model_run_artifact_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VW6XMyHtxirp"
      },
      "source": [
        "### Evaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6k42yS9Cxl1r"
      },
      "source": [
        "- `Evaluator` 組件可評估模型性能。\n",
        "- `Evaluator` 組件為 [TensorFlow Model Analysis (TFMA)](https://www.tensorflow.org/tfx/model_analysis/get_started) 模組功能。 \n",
        "- `Evaluator` 可以設定門檻值以比較並選擇較佳的模型。這在生產管道設置中很有用，您可以每天自動訓練和驗證模型。\n",
        "- `Evaluator` 的輸入:\n",
        "  - 輸入資料集來自 `ExampleGen`。\n",
        "  - 訓練模型來自 `Trainer` 和切片配置。切片配置允許您根據特徵值對指標進行切片（例如，您的模型在早上 8 點和晚上 8 點開始的出租車行程中表現如何？）。\n",
        "- 在此筆記本範例只訓練一個模型，所以`Evaluator`自動將模型標記為“Good”。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVhfzzh9PDEx"
      },
      "source": [
        "# Imported files such as taxi_constants are normally cached, so changes are\n",
        "# not honored after the first import.  Normally this is good for efficiency, but\n",
        "# during development when we may be iterating code it can be a problem. To\n",
        "# avoid this problem during development, reload the file.\n",
        "import taxi_constants\n",
        "import sys\n",
        "if 'google.colab' in sys.modules:  # Testing to see if we're doing development\n",
        "  import importlib\n",
        "  importlib.reload(taxi_constants)\n",
        "\n",
        "eval_config = tfma.EvalConfig(\n",
        "    model_specs=[\n",
        "        # This assumes a serving model with signature 'serving_default'. If\n",
        "        # using estimator based EvalSavedModel, add signature_name: 'eval' and\n",
        "        # remove the label_key.\n",
        "        tfma.ModelSpec(\n",
        "            signature_name='serving_default',\n",
        "            label_key=taxi_constants.LABEL_KEY,\n",
        "            preprocessing_function_names=['transform_features'],\n",
        "            )\n",
        "        ],\n",
        "    metrics_specs=[\n",
        "        tfma.MetricsSpec(\n",
        "            # The metrics added here are in addition to those saved with the\n",
        "            # model (assuming either a keras model or EvalSavedModel is used).\n",
        "            # Any metrics added into the saved model (for example using\n",
        "            # model.compile(..., metrics=[...]), etc) will be computed\n",
        "            # automatically.\n",
        "            # To add validation thresholds for metrics saved with the model,\n",
        "            # add them keyed by metric name to the thresholds map.\n",
        "            metrics=[\n",
        "                tfma.MetricConfig(class_name='ExampleCount'),\n",
        "                tfma.MetricConfig(class_name='BinaryAccuracy',\n",
        "                  threshold=tfma.MetricThreshold(\n",
        "                      value_threshold=tfma.GenericValueThreshold(\n",
        "                          lower_bound={'value': 0.5}),\n",
        "                      # Change threshold will be ignored if there is no\n",
        "                      # baseline model resolved from MLMD (first run).\n",
        "                      change_threshold=tfma.GenericChangeThreshold(\n",
        "                          direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
        "                          absolute={'value': -1e-10})))\n",
        "            ]\n",
        "        )\n",
        "    ],\n",
        "    slicing_specs=[\n",
        "        # An empty slice spec means the overall slice, i.e. the whole dataset.\n",
        "        tfma.SlicingSpec(),\n",
        "        # Data can be sliced along a feature column. In this case, data is\n",
        "        # sliced along feature column trip_start_hour.\n",
        "        tfma.SlicingSpec(\n",
        "            feature_keys=['trip_start_hour'])\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zjcx8g6mihSt"
      },
      "source": [
        "# Use TFMA to compute a evaluation statistics over features of a model and\n",
        "# validate them against a baseline.\n",
        "\n",
        "# The model resolver is only required if performing model validation in addition\n",
        "# to evaluation. In this case we validate against the latest blessed model. If\n",
        "# no model has been blessed before (as in this case) the evaluator will make our\n",
        "# candidate the first blessed model.\n",
        "model_resolver = tfx.dsl.Resolver(\n",
        "      strategy_class=tfx.dsl.experimental.LatestBlessedModelStrategy,\n",
        "      model=tfx.dsl.Channel(type=tfx.types.standard_artifacts.Model),\n",
        "      model_blessing=tfx.dsl.Channel(\n",
        "          type=tfx.types.standard_artifacts.ModelBlessing)).with_id(\n",
        "              'latest_blessed_model_resolver')\n",
        "context.run(model_resolver, enable_cache=True)\n",
        "\n",
        "evaluator = tfx.components.Evaluator(\n",
        "    examples=example_gen.outputs['examples'],\n",
        "    model=trainer.outputs['model'],\n",
        "    baseline_model=model_resolver.outputs['model'],\n",
        "    eval_config=eval_config)\n",
        "context.run(evaluator, enable_cache=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4GghePOTJxL"
      },
      "source": [
        "evaluator.outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U729j5X5QQUQ"
      },
      "source": [
        "context.show(evaluator.outputs['evaluation'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-tI4p6m-OAn"
      },
      "source": [
        "- 要切片顯示模型情形，需使用 TFMA 模組。\n",
        "- 在此示範將`trip_start_hour`切片視覺化，TFMA 支援許多其他可視化，例如公平指標和繪製模型性能的時間序列。要了解更多信息，請參閱[教學](https://www.tensorflow.org/tfx/tutorials/model_analysis/tfma_basic)。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyis6iy0HLdi"
      },
      "source": [
        "import tensorflow_model_analysis as tfma\n",
        "\n",
        "# Get the TFMA output result path and load the result.\n",
        "PATH_TO_RESULT = evaluator.outputs['evaluation'].get()[0].uri\n",
        "tfma_result = tfma.load_eval_result(PATH_TO_RESULT)\n",
        "\n",
        "# Show data sliced along feature column trip_start_hour.\n",
        "tfma.view.render_slicing_metrics(\n",
        "    tfma_result, slicing_column='trip_start_hour')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UA1F-zo13fdX"
      },
      "source": [
        "- 通過門檻值的模型會得到祝福 `blessing` ，第一次預設會自動取得，之後持續訓練過程會將取得祝福的模型再上線。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZmiRtg6TKtR"
      },
      "source": [
        "blessing_uri = evaluator.outputs['blessing'].get()[0].uri\n",
        "!ls -l {blessing_uri}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2iuEwE14I3Z"
      },
      "source": [
        "- 現在也可以讀取經過驗證成功的紀錄。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxa5G08bSJ8a"
      },
      "source": [
        "PATH_TO_RESULT = evaluator.outputs['evaluation'].get()[0].uri\n",
        "print(tfma.load_validation_result(PATH_TO_RESULT))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dRB_xGh4t_N"
      },
      "source": [
        "### Pusher"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqy2wG4t4wH0"
      },
      "source": [
        "- `Pusher` 組件通常位於 TFX 管道末端。\n",
        "- `Pusher` 組件檢查模型是否已通過驗證，如果是，則將模型導出至 \n",
        "`_serving_model_dir`。\n",
        "- `Pusher` 將以 `SavedModel` 格式導出您的模型。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r45nQ69eikc9"
      },
      "source": [
        "pusher = tfx.components.Pusher(\n",
        "    model=trainer.outputs['model'],\n",
        "    model_blessing=evaluator.outputs['blessing'],\n",
        "    push_destination=tfx.proto.PushDestination(\n",
        "        filesystem=tfx.proto.PushDestination.Filesystem(\n",
        "            base_directory=_serving_model_dir)))\n",
        "context.run(pusher, enable_cache=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRkWo-MzTSss"
      },
      "source": [
        "pusher.outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zyIqWl9TSdG"
      },
      "source": [
        "push_uri = pusher.outputs['pushed_model'].get()[0].uri\n",
        "model = tf.saved_model.load(push_uri)\n",
        "\n",
        "for item in model.signatures.items():\n",
        "  pp.pprint(item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-YPNUuHANtj"
      },
      "source": [
        "終於完成 TFX 所有組件的示範!"
      ]
    }
  ]
}