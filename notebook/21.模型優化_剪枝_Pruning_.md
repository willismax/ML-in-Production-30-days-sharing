---
jupyter:
  jupytext:
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.15.1
  kernelspec:
    display_name: Python 3
    name: python3
---

<!-- #region id="view-in-github" colab_type="text" -->
<a href="https://colab.research.google.com/github/willismax/ML-in-Production-30-days-sharing/blob/main/notebook/21.%E6%A8%A1%E5%9E%8B%E5%84%AA%E5%8C%96_%E5%89%AA%E6%9E%9D_Pruning_.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
<!-- #endregion -->

<!-- #region id="fioAy54REFpk" -->
# 剪枝 Pruning
<!-- #endregion -->

<!-- #region id="6YV2WnBDdQwO" -->
- 此為鐵人賽系列文示範文件，參考[TensorFlow Lite官方範例](https://www.tensorflow.org/lite/performance/post_training_quantization)修改而成。
- TF Lite 評估函數參考[來源](https://www.tensorflow.org/lite/performance/post_training_integer_quant_16x8)。
<!-- #endregion -->

<!-- #region id="SwvaMflTYNgo" -->
- 剪枝 [Pruning](https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras)將無關緊要的權重歸零刪除歸零，在壓縮時能明顯縮小尺寸。
- 經過剪枝且量化的模型可以縮小的原來1/10大小。
- Tensorflow 模型優化模組的`prune_low_magnitude()`，可以將Keras模型在訓練期間將影響較小的權重修剪歸零。
- 在本範例中，您將使用與示範[訓練後量化](https://colab.research.google.com/drive/1ukgVrMdtWjpReIygWHJ7-Lcw61Lv5kAO)相同的基準模型進行優化。
<!-- #endregion -->

```python id="_eLMjw2wdKLg"
# 建立評估模型的dict
MODEL_SIZE = {}
ACCURACY = {}
```

```python id="6hlLho0edo60" colab={"base_uri": "https://localhost:8080/"} outputId="ae4f479a-ba33-4874-84ca-316919ea21dc"
!pip install -q -U tensorflow_model_optimization
```

```python id="9FWoN81ud8Pl"
import tensorflow as tf
import tensorflow_model_optimization as tfmot
import numpy as np
import os
```

<!-- #region id="b7irjO4VkU5U" -->
## 建立基本模型
<!-- #endregion -->

<!-- #region id="VBpUaNvRkYdI" -->
- 模型採用`tf.keras.datasets.mnist`，用CNN進行建模。
<!-- #endregion -->

```python colab={"base_uri": "https://localhost:8080/"} id="u6rKXfkpd6lN" outputId="771894ab-4bdb-4bc9-ef39-9ea97ab51833"
# Load MNIST dataset
mnist = tf.keras.datasets.mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

# Normalize the input image so that each pixel value is between 0 to 1.
train_images = train_images / 255.0
test_images = test_images / 255.0
```

```python id="9YdaJpyKeYFG"
def model_builder():

  keras = tf.keras

  model = keras.Sequential([
    keras.layers.InputLayer(input_shape=(28, 28)),
    keras.layers.Reshape(target_shape=(28, 28, 1)),
    keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),
    keras.layers.MaxPooling2D(pool_size=(2, 2)),
    keras.layers.Flatten(),
    keras.layers.Dense(10, activation='softmax')
  ])

  return model
```

```python colab={"base_uri": "https://localhost:8080/"} id="nnjoU2Kvd7Qd" outputId="40d0a807-7239-44f8-f937-32498009505e"
baseline_model = model_builder()
baseline_model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
    )

baseline_model.summary()
baseline_model.save_weights('baseline_weights.h5')

baseline_model.fit(
    train_images, 
    train_labels, 
    epochs=1, 
    shuffle=False
    )
```

```python colab={"base_uri": "https://localhost:8080/"} id="acWJOg5vkeka" outputId="064f6562-7b21-4b7d-ef0f-cb905e161c64"
# 儲存未量化模型
baseline_model.save('non_pruned.h5', include_optimizer=False)

# 評估模型並紀錄準確率
_, ACCURACY['baseline Keras model'] = baseline_model.evaluate(test_images, test_labels)

# 紀錄模型大小
MODEL_SIZE['baseline h5'] = os.path.getsize('non_pruned.h5')
```

```python colab={"base_uri": "https://localhost:8080/"} id="14zo30Eqke6R" outputId="53c83dad-99cf-42c1-ec4a-1fc83e52dadd"
ACCURACY
```

```python colab={"base_uri": "https://localhost:8080/"} id="nGT8PhHXtTiz" outputId="7499f416-7dcc-4fc1-be22-55d5f2df8f98"
MODEL_SIZE
```

<!-- #region id="oSBeouK9tmCs" -->
## 使用剪枝調整模型
<!-- #endregion -->

<!-- #region id="2J0MhxPbhyJN" -->
- 進行剪枝，另外因為剪枝模型方法有增加一層包裝層，摘要顯示的參數會增加。
<!-- #endregion -->

```python id="TpqizJsKYPBA" colab={"base_uri": "https://localhost:8080/"} outputId="312c7436-e40d-45cd-86bc-9d67023058d3"
# Get the pruning method
prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude

# Compute end step to finish pruning after 2 epochs.
batch_size = 128
epochs = 2
validation_split = 0.1 # 10% of training set will be used for validation set. 

num_images = train_images.shape[0] * (1 - validation_split)
end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs

# Define pruning schedule.
pruning_params = {
    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(
        initial_sparsity=0.50,
        final_sparsity=0.80,
        begin_step=0,
        end_step=end_step)
    }

# Pass in the trained baseline model
model_for_pruning = prune_low_magnitude(
    baseline_model, 
    **pruning_params
    )

# `prune_low_magnitude` requires a recompile.
model_for_pruning.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
    )

model_for_pruning.summary()
```

<!-- #region id="qgmHaZI6fip_" -->
- 查看模型中某一層的權重。
  - 剪枝前，有些微弱的權重。
  - 剪枝後，其中許多將被清零。
<!-- #endregion -->

```python id="y5ekdEBigB5l" colab={"base_uri": "https://localhost:8080/"} outputId="718edf8a-5213-4677-8ede-27ee6b3d6587"
# 剪枝前的模型權重
model_for_pruning.weights[1]
```

<!-- #region id="jpL0rLsMO0Fo" -->
- 重新訓練模型。並在 Callback 增加`tfmot.sparsity.keras.UpdatePruningStep()`參數。
<!-- #endregion -->

```python id="DUCz6PL371Bx" colab={"base_uri": "https://localhost:8080/"} outputId="e648d669-67f8-408f-e5a4-2f6731dd47e0"
# Callback to update pruning wrappers at each step
callbacks=[tfmot.sparsity.keras.UpdatePruningStep()]

# Train and prune the model
model_for_pruning.fit(
    train_images, 
    train_labels,
    epochs=epochs, 
    validation_split=validation_split,
    callbacks=callbacks
    )
```

<!-- #region id="5pSmR6iHPPOH" -->
- 重新訓練後已修剪，觀察同一層的權重變化，許多不重要的權重已歸零。
<!-- #endregion -->

```python id="TOK4TidJhXpT" colab={"base_uri": "https://localhost:8080/"} outputId="c88aef84-2029-4b0f-b1f2-9b4dbd12595c"
# 剪枝後的模型權重
model_for_pruning.weights[1]
```

<!-- #region id="q8o0ukVLmV4o" -->
### 剪枝後移除包裝層

<!-- #endregion -->

<!-- #region id="oVo5HSttPq5D" -->
- 剪枝之後，您可以用[`tfmot.sparsity.keras.strip_pruning()`](https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/strip_pruning)刪除包裝層以具有與基線模型相同的層和參數。
- 此方法也有助於保存模型並導出為`*.tflite`檔案格式。
<!-- #endregion -->

```python id="PbfLhZv68vwc" colab={"base_uri": "https://localhost:8080/"} outputId="63c53618-ec3e-403b-f1ff-a28003861df9"
# Remove pruning wrappers
model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)
model_for_export    .summary()
```

<!-- #region id="UitndoqKQd-G" -->
- 因為包裝器已被移除，相同的模型權重，已移置索引[0]。
<!-- #endregion -->

```python id="SG6-aF9yiraG" colab={"base_uri": "https://localhost:8080/"} outputId="ef58054c-64aa-4593-f02e-790f087cb8a8"
model_for_export.weights[0]
```

<!-- #region id="teMWEkNFQvVk" -->
- 將剪枝後的檔案保存為`*.h5`，此時模型與修剪前大小相同。但一旦壓縮模型則改善
相當明顯。


<!-- #endregion -->

```python id="CjjDMqJCTjqz" colab={"base_uri": "https://localhost:8080/"} outputId="c403601e-bb12-4d0d-9fb3-4f187a6ec4a4"
# Save Keras model
model_for_export.save('pruned_model.h5', include_optimizer=False)

# Get uncompressed model size of baseline and pruned models
MODEL_SIZE['pruned non quantized h5'] = os.path.getsize('pruned_model.h5')
```

```python colab={"base_uri": "https://localhost:8080/"} id="ywvDyXjfGW8s" outputId="df06ddbc-92c1-4b9c-89b5-116856d4078c"
MODEL_SIZE
```

<!-- #region id="EwlnW7KWeVxl" -->
## 模型壓縮3倍術
<!-- #endregion -->

<!-- #region id="dBvVeyAiRpzi" -->
- 剪枝後的模型再壓縮。
- 壓縮後檔案大小約為原本1/3，這是因為剪枝後歸零的權重可以更有效的壓縮。
<!-- #endregion -->

```python id="GlUv5yXHSEOr"
import tempfile
import zipfile

_, zipped_file = tempfile.mkstemp('.zip')
with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:
    f.write('pruned_model.h5')


MODEL_SIZE['pruned non quantized h5'] = os.path.getsize('pruned_model.h5')
```

```python id="VWQ_AgiX_yiP" colab={"base_uri": "https://localhost:8080/"} outputId="fd42131a-6009-4c92-b4c7-8445ed12dbd6"
MODEL_SIZE
```

<!-- #region id="9ddwmhlAX4ug" -->
## 模型壓縮10倍術

<!-- #endregion -->

<!-- #region id="5Fp8d4t-dgnj" -->
- 現在嘗試將已精剪枝後的模型再量化。
- 量化原本就會縮小約4倍，將剪枝模型壓縮後再量化，與基本模型相比，這使模型減少了約 10 倍。
- 小10倍精度還能維持水準。
<!-- #endregion -->

```python colab={"base_uri": "https://localhost:8080/"} id="FTK3Ulja-dUy" outputId="2c80a8f1-cae7-45b8-f83d-168e1fb3a8eb"
# 剪枝壓縮後再量化模型
converter = tf.lite.TFLiteConverter.from_keras_model(baseline_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]

tflite_model = converter.convert()

with open('pruned_quantized.tflite', 'wb') as f:
    f.write(tflite_model)
```

```python id="qIY6n9XWCvt5" colab={"base_uri": "https://localhost:8080/"} outputId="adb4e900-c8cb-4003-f86e-d643e63ad9cc"
MODEL_SIZE['pruned quantized tflite'] = os.path.getsize('pruned_quantized.tflite')
MODEL_SIZE

```

<!-- #region id="v4ytiH3ynIid" -->
- 即便小十倍，精度還維持原本水準。
<!-- #endregion -->

```python id="6d4MY-ozBRRd"
# A helper function to evaluate the TF Lite model using "test" dataset.
# from: https://www.tensorflow.org/lite/performance/post_training_integer_quant_16x8#evaluate_the_models
def evaluate_model(filemane):
  #Load the model into the interpreters
  interpreter = tf.lite.Interpreter(model_path=str(filemane))
  interpreter.allocate_tensors()

  input_index = interpreter.get_input_details()[0]["index"]
  output_index = interpreter.get_output_details()[0]["index"]

  # Run predictions on every image in the "test" dataset.
  prediction_digits = []
  for test_image in test_images:
    # Pre-processing: add batch dimension and convert to float32 to match with
    # the model's input data format.
    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)
    interpreter.set_tensor(input_index, test_image)

    # Run inference.
    interpreter.invoke()

    # Post-processing: remove batch dimension and find the digit with highest
    # probability.
    output = interpreter.tensor(output_index)
    digit = np.argmax(output()[0])
    prediction_digits.append(digit)

  # Compare prediction results with ground truth labels to calculate accuracy.
  accurate_count = 0
  for index in range(len(prediction_digits)):
    if prediction_digits[index] == test_labels[index]:
      accurate_count += 1
  accuracy = accurate_count * 1.0 / len(prediction_digits)

  return accuracy
```

```python id="PZBAdJmuWN0A" colab={"base_uri": "https://localhost:8080/"} outputId="b2324bef-79bc-45d0-ddf5-19a279bcc55a"
# Get accuracy of pruned Keras and TF Lite models

_, ACCURACY['pruned model h5'] = model_for_pruning.evaluate(test_images, test_labels)
ACCURACY['pruned and quantized tflite'] = evaluate_model('pruned_quantized.tflite')
```

<!-- #region id="jfxlrwVJfLlt" -->
## 成果
<!-- #endregion -->

```python colab={"base_uri": "https://localhost:8080/"} id="bamu10fzA0R_" outputId="0475f49c-b177-41a7-a76a-3578ea7e3bf2"
ACCURACY
```

```python colab={"base_uri": "https://localhost:8080/"} id="lPJh07O1fOwh" outputId="d7b22637-6b35-4e50-bec5-cc353d9de4ba"
MODEL_SIZE
```

<!-- #region id="JDVfBxbVfSiz" -->
## 參考

<!-- #endregion -->

<!-- #region id="PJ9OnRQDfYQD" -->
- [TensorFlow Lite官方範例](https://www.tensorflow.org/lite/performance/post_training_quantization)。
- TF Lite 評估函數參考[來源](https://www.tensorflow.org/lite/performance/post_training_integer_quant_16x8)。
<!-- #endregion -->
