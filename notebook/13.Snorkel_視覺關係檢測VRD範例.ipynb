{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "tags,-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Snorkel-視覺關係檢測VRD範例.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "29fc13409f38487a8b05ef4255ecbac6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_769e9a17b88d47fda1aee2a3ed739190",
              "IPY_MODEL_31f90659a5eb42adb7c322a0b8578ca6",
              "IPY_MODEL_59c2834a97aa4a888e6cc741b4028319"
            ],
            "layout": "IPY_MODEL_4a65825500944be48d40d96a869d85b2"
          }
        },
        "769e9a17b88d47fda1aee2a3ed739190": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52facabac00c49ceb92633884c1957c2",
            "placeholder": "​",
            "style": "IPY_MODEL_9552000d9ac54672b127d04bdad9ffa0",
            "value": "100%"
          }
        },
        "31f90659a5eb42adb7c322a0b8578ca6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ee2e86fd90e4cd88d661532b6664dbd",
            "max": 46830571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3cc37e65ec69421c8b10f5ed0a86b9b2",
            "value": 46830571
          }
        },
        "59c2834a97aa4a888e6cc741b4028319": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b9f1136c5b24d949c7e796d549979cb",
            "placeholder": "​",
            "style": "IPY_MODEL_7ae2c7366ecb4162a314b11dd6f1808f",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 69.4MB/s]"
          }
        },
        "4a65825500944be48d40d96a869d85b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52facabac00c49ceb92633884c1957c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9552000d9ac54672b127d04bdad9ffa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ee2e86fd90e4cd88d661532b6664dbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cc37e65ec69421c8b10f5ed0a86b9b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9b9f1136c5b24d949c7e796d549979cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ae2c7366ecb4162a314b11dd6f1808f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/willismax/ML-in-Production-30-days-sharing/blob/main/notebook/13.Snorkel_%E8%A6%96%E8%A6%BA%E9%97%9C%E4%BF%82%E6%AA%A2%E6%B8%ACVRD%E7%AF%84%E4%BE%8B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BodGnI3H1an"
      },
      "source": [
        "# Visual Relationship Detection 視覺關係偵測\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQy4NnfBKnTX"
      },
      "source": [
        "- 此範例源自 [snorkel-tutorials](https://github.com/snorkel-team/snorkel-tutorials/blob/master/visual_relation/visual_relation_tutorial.ipynb)，目的為對[視覺關係檢測 (VRD) 數據集](https://cs.stanford.edu/people/ranjaykrishna/vrd/)進行操作，專注於圖片內物件之間的關係分類任務。\n",
        "- 通常圖片內容物都有物體之間的關聯性，定義描述為為`a subject <predictate> object`。\n",
        "    - 例如，`person riding bicycle`，“person”和“bicycle”分別是主詞和受詞，“riding”是關係動詞。\n",
        "- 以下圖示紅色框代表主題，而綠色框代表對象。該主詞（如踢）表示什麼關係連接主體和客體。\n",
        "\n",
        "![](https://i.imgur.com/SMxT2C4.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTB3Rcx6psCR"
      },
      "source": [
        "## 設定專案環境"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejRrdVWFpytA"
      },
      "source": [
        "複製專案"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_b1pYFwWsJS"
      },
      "source": [
        "!git clone https://github.com/snorkel-team/snorkel-tutorials.git > clone_log.txt\n",
        "!pip3 install snorkel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd snorkel-tutorials/visual_relation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUbm69II0BLc",
        "outputId": "7a3d98d4-a35a-4230-c2c4-b20f023c821e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/snorkel-tutorials/visual_relation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4b-fjO9j0gU"
      },
      "source": [
        "*改寫專案檔案\n",
        "- 因範例採用舊的pandas，已棄用的`df.as_matrix()`還在範例程式`snorkel-tutorials/visual_relation/model.py`中，為了符合現今環境，將改寫為`df.values` (model.py第135行附近)。\n",
        "- 您可以直接執行以下`%%writefile`指令，或自行循路徑修正檔案。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIc1SUj3fqPP",
        "outputId": "a8f58983-98c3-4774-da83-006ad0f50c01"
      },
      "source": [
        "%%writefile /content/snorkel-tutorials/visual_relation/model.py\n",
        "\n",
        "# fix _get_wordvec() df.as_matrix() to df.values (line:137)\n",
        "import csv\n",
        "from pathlib import Path\n",
        "from typing import Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "from snorkel.analysis import Scorer\n",
        "from snorkel.classification import DictDataset, MultitaskClassifier, Operation, Task\n",
        "from snorkel.classification.data import XDict, YDict\n",
        "\n",
        "\n",
        "def union(bbox1, bbox2):\n",
        "    \"\"\"Create the union of the two bboxes.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    bbox1\n",
        "        Coordinates of first bounding box\n",
        "    bbox2\n",
        "        Coordinates of second bounding box\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    [y0, y1, x0, x1]\n",
        "        Coordinates of union of input bounding boxes\n",
        "\n",
        "    \"\"\"\n",
        "    y0 = min(bbox1[0], bbox2[0])\n",
        "    y1 = max(bbox1[1], bbox2[1])\n",
        "    x0 = min(bbox1[2], bbox2[2])\n",
        "    x1 = max(bbox1[3], bbox2[3])\n",
        "    return [y0, y1, x0, x1]\n",
        "\n",
        "\n",
        "def crop_img_arr(img_arr, bbox):\n",
        "    \"\"\"Crop bounding box from image.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    img_arr\n",
        "        Image in array format\n",
        "    bbox\n",
        "        Coordinates of bounding box to crop\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    img_arr\n",
        "        Cropped image\n",
        "\n",
        "    \"\"\"\n",
        "    return img_arr[bbox[0] : bbox[1], bbox[2] : bbox[3], :]\n",
        "\n",
        "\n",
        "class SceneGraphDataset(DictDataset):\n",
        "    \"\"\"Dataloader for Scene Graph Dataset.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        name: str,\n",
        "        split: str,\n",
        "        image_dir: str,\n",
        "        df: pandas.DataFrame,\n",
        "        image_size=224,\n",
        "    ) -> None:\n",
        "        self.image_dir = Path(image_dir)\n",
        "        X_dict = {\n",
        "            \"img_fn\": df[\"source_img\"].tolist(),\n",
        "            \"obj_bbox\": df[\"object_bbox\"].tolist(),\n",
        "            \"sub_bbox\": df[\"subject_bbox\"].tolist(),\n",
        "            \"obj_category\": df[\"object_category\"].tolist(),\n",
        "            \"sub_category\": df[\"subject_category\"].tolist(),\n",
        "        }\n",
        "        Y_dict = {\n",
        "            \"visual_relation_task\": torch.LongTensor(df[\"label\"].to_numpy())\n",
        "        }  # change to take in the rounded train labels\n",
        "        super(SceneGraphDataset, self).__init__(name, split, X_dict, Y_dict)\n",
        "\n",
        "        # define standard set of transformations to apply to each image\n",
        "        self.transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize((image_size, image_size)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(\n",
        "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        "                ),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def __getitem__(self, index: int) -> Tuple[XDict, YDict]:\n",
        "        img_fn = self.X_dict[\"img_fn\"][index]\n",
        "        img_arr = np.array(Image.open(self.image_dir / img_fn))\n",
        "\n",
        "        obj_bbox = self.X_dict[\"obj_bbox\"][index]\n",
        "        sub_bbox = self.X_dict[\"sub_bbox\"][index]\n",
        "        obj_category = self.X_dict[\"obj_category\"][index]\n",
        "        sub_category = self.X_dict[\"sub_category\"][index]\n",
        "\n",
        "        # compute crops\n",
        "        obj_crop = crop_img_arr(img_arr, obj_bbox)\n",
        "        sub_crop = crop_img_arr(img_arr, sub_bbox)\n",
        "        union_crop = crop_img_arr(img_arr, union(obj_bbox, sub_bbox))\n",
        "\n",
        "        # transform each crop\n",
        "        x_dict = {\n",
        "            \"obj_crop\": self.transform(Image.fromarray(obj_crop)),\n",
        "            \"sub_crop\": self.transform(Image.fromarray(sub_crop)),\n",
        "            \"union_crop\": self.transform(Image.fromarray(union_crop)),\n",
        "            \"obj_category\": obj_category,\n",
        "            \"sub_category\": sub_category,\n",
        "        }\n",
        "\n",
        "        y_dict = {name: label[index] for name, label in self.Y_dict.items()}\n",
        "        return x_dict, y_dict\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X_dict[\"img_fn\"])\n",
        "\n",
        "\n",
        "class WordEmb(nn.Module):\n",
        "    \"\"\"Extract and concat word embeddings for obj and sub categories.\"\"\"\n",
        "\n",
        "    def __init__(self, glove_fn=\"data/glove/glove.6B.100d.txt\"):\n",
        "        super(WordEmb, self).__init__()\n",
        "\n",
        "        self.word_embs = pandas.read_csv(\n",
        "            glove_fn, sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE\n",
        "        )\n",
        "\n",
        "    def _get_wordvec(self, word):\n",
        "        return self.word_embs.loc[word].values\n",
        "\n",
        "    def forward(self, obj_category, sub_category):\n",
        "        obj_emb = self._get_wordvec(obj_category)\n",
        "        sub_emb = self._get_wordvec(sub_category)\n",
        "        embs = np.concatenate([obj_emb, sub_emb], axis=1)\n",
        "        return torch.FloatTensor(embs)\n",
        "\n",
        "\n",
        "# Classes and helper functions for defining classifier\n",
        "def init_fc(fc):\n",
        "    torch.nn.init.xavier_uniform_(fc.weight)\n",
        "    fc.bias.data.fill_(0.01)\n",
        "\n",
        "\n",
        "class FlatConcat(nn.Module):\n",
        "    \"\"\"Module that flattens and concatenates features\"\"\"\n",
        "\n",
        "    def forward(self, *inputs):\n",
        "        return torch.cat([input.view(input.size(0), -1) for input in inputs], dim=1)\n",
        "\n",
        "\n",
        "# Helper functions to geenerate operations\n",
        "def get_op_sequence():\n",
        "    # define feature extractors for each of the (union, subject, and object) image crops\n",
        "    union_feat_op = Operation(\n",
        "        name=\"union_feat_op\",\n",
        "        module_name=\"feat_extractor\",\n",
        "        inputs=[(\"_input_\", \"union_crop\")],\n",
        "    )\n",
        "\n",
        "    sub_feat_op = Operation(\n",
        "        name=\"sub_feat_op\",\n",
        "        module_name=\"feat_extractor\",\n",
        "        inputs=[(\"_input_\", \"sub_crop\")],\n",
        "    )\n",
        "\n",
        "    obj_feat_op = Operation(\n",
        "        name=\"obj_feat_op\",\n",
        "        module_name=\"feat_extractor\",\n",
        "        inputs=[(\"_input_\", \"obj_crop\")],\n",
        "    )\n",
        "\n",
        "    # define an operation to extract word embeddings for subject and object categories\n",
        "    word_emb_op = Operation(\n",
        "        name=\"word_emb_op\",\n",
        "        module_name=\"word_emb\",\n",
        "        inputs=[(\"_input_\", \"sub_category\"), (\"_input_\", \"obj_category\")],\n",
        "    )\n",
        "\n",
        "    # define an operation to concatenate image features and word embeddings\n",
        "    concat_op = Operation(\n",
        "        name=\"concat_op\",\n",
        "        module_name=\"feat_concat\",\n",
        "        inputs=[\"obj_feat_op\", \"sub_feat_op\", \"union_feat_op\", \"word_emb_op\"],\n",
        "    )\n",
        "\n",
        "    # define an operation to make a prediction over all concatenated features\n",
        "    prediction_op = Operation(\n",
        "        name=\"head_op\", module_name=\"prediction_head\", inputs=[\"concat_op\"]\n",
        "    )\n",
        "\n",
        "    return [\n",
        "        sub_feat_op,\n",
        "        obj_feat_op,\n",
        "        union_feat_op,\n",
        "        word_emb_op,\n",
        "        concat_op,\n",
        "        prediction_op,\n",
        "    ]\n",
        "\n",
        "\n",
        "# Create model from pre loaded resnet cnn.\n",
        "def create_model(resnet_cnn):\n",
        "    # freeze the resnet weights\n",
        "    for param in resnet_cnn.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # define input features\n",
        "    in_features = resnet_cnn.fc.in_features\n",
        "    feature_extractor = nn.Sequential(*list(resnet_cnn.children())[:-1])\n",
        "\n",
        "    # initialize FC layer: maps 3 sets of image features to class logits\n",
        "    WEMB_SIZE = 100\n",
        "    fc = nn.Linear(in_features * 3 + 2 * WEMB_SIZE, 3)\n",
        "    init_fc(fc)\n",
        "\n",
        "    # define layers\n",
        "    module_pool = nn.ModuleDict(\n",
        "        {\n",
        "            \"feat_extractor\": feature_extractor,\n",
        "            \"prediction_head\": fc,\n",
        "            \"feat_concat\": FlatConcat(),\n",
        "            \"word_emb\": WordEmb(),\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # define task flow through modules\n",
        "    op_sequence = get_op_sequence()\n",
        "    pred_cls_task = Task(\n",
        "        name=\"visual_relation_task\",\n",
        "        module_pool=module_pool,\n",
        "        op_sequence=op_sequence,\n",
        "        scorer=Scorer(metrics=[\"f1_micro\"]),\n",
        "    )\n",
        "    return MultitaskClassifier([pred_cls_task])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/snorkel-tutorials/visual_relation/model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2qJYYD8r8L-"
      },
      "source": [
        "## 1. 加載數據集\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hFQT3AfH1aw"
      },
      "source": [
        "\n",
        "下載 [VRD 數據集]((https://cs.stanford.edu/people/ranjaykrishna/vrd/))並過濾包含至少一個動作謂詞的圖像，因為這些比幾何關係更難分類，如above或next to。\n",
        "- 範例將訓練集、有效集和測試集加載為DataFrame：\n",
        "    - `label`: 對象之間的關係。`0: RIDE, 1: CARRY, 2:OTHER動作謂詞`\n",
        "    - `object_bbox`:  `[ymin, ymax, xmin, xmax]`\n",
        "    - `object_category`\n",
        "    - `source_img`\n",
        "    - `subject_bbox`:  `[ymin, ymax, xmin, xmax]`\n",
        "    - `subject_category`\n",
        "- 數據集的採樣版本在訓練集、開發集和測試集上使用相同的 26 個數據。此設置旨在快速演示 Snorkel 如何處理此任務，而不是演示性能。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJEZ-kDdH1ay",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "514d83c9-608c-4545-d4b3-3afa13d0db57"
      },
      "source": [
        "import os\n",
        "from utils import load_vrd_data\n",
        "\n",
        "# setting sample=False will take ~3 hours to run (downloads full VRD dataset)\n",
        "sample = True\n",
        "is_test = os.environ.get(\"TRAVIS\") == \"true\" or os.environ.get(\"IS_TEST\") == \"true\"\n",
        "df_train, df_valid, df_test = load_vrd_data(sample, is_test)\n",
        "\n",
        "print(\"Train Relationships: \", len(df_train))\n",
        "print(\"Dev Relationships: \", len(df_valid))\n",
        "print(\"Test Relationships: \", len(df_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Relationships:  26\n",
            "Dev Relationships:  26\n",
            "Test Relationships:  26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ZCcyXOT4tyvG",
        "outputId": "714745db-5bfe-4d3d-d030-78c380682017"
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  subject_category object_category          subject_bbox  \\\n",
              "0           person      skateboard    [46, 394, 59, 376]   \n",
              "1           person            ramp  [111, 598, 354, 603]   \n",
              "2         umbrella          person  [188, 257, 383, 463]   \n",
              "3         umbrella          person  [188, 257, 383, 463]   \n",
              "4         umbrella          person  [196, 367, 370, 444]   \n",
              "\n",
              "            object_bbox  label                   source_img  \n",
              "0   [231, 397, 55, 191]      0  4788767927_236444fd03_b.jpg  \n",
              "1   [472, 1023, 1, 682]      2  4357208606_ce0f096a70_b.jpg  \n",
              "2  [196, 367, 370, 444]      2  2820743071_dae064f6e7_o.jpg  \n",
              "3  [196, 367, 370, 444]      2  2820743071_dae064f6e7_o.jpg  \n",
              "4  [188, 257, 383, 463]      2  2820743071_dae064f6e7_o.jpg  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e11a960d-8291-4e32-b890-80e8ecd97660\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subject_category</th>\n",
              "      <th>object_category</th>\n",
              "      <th>subject_bbox</th>\n",
              "      <th>object_bbox</th>\n",
              "      <th>label</th>\n",
              "      <th>source_img</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>person</td>\n",
              "      <td>skateboard</td>\n",
              "      <td>[46, 394, 59, 376]</td>\n",
              "      <td>[231, 397, 55, 191]</td>\n",
              "      <td>0</td>\n",
              "      <td>4788767927_236444fd03_b.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>person</td>\n",
              "      <td>ramp</td>\n",
              "      <td>[111, 598, 354, 603]</td>\n",
              "      <td>[472, 1023, 1, 682]</td>\n",
              "      <td>2</td>\n",
              "      <td>4357208606_ce0f096a70_b.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>umbrella</td>\n",
              "      <td>person</td>\n",
              "      <td>[188, 257, 383, 463]</td>\n",
              "      <td>[196, 367, 370, 444]</td>\n",
              "      <td>2</td>\n",
              "      <td>2820743071_dae064f6e7_o.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>umbrella</td>\n",
              "      <td>person</td>\n",
              "      <td>[188, 257, 383, 463]</td>\n",
              "      <td>[196, 367, 370, 444]</td>\n",
              "      <td>2</td>\n",
              "      <td>2820743071_dae064f6e7_o.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>umbrella</td>\n",
              "      <td>person</td>\n",
              "      <td>[196, 367, 370, 444]</td>\n",
              "      <td>[188, 257, 383, 463]</td>\n",
              "      <td>2</td>\n",
              "      <td>2820743071_dae064f6e7_o.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e11a960d-8291-4e32-b890-80e8ecd97660')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e11a960d-8291-4e32-b890-80e8ecd97660 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e11a960d-8291-4e32-b890-80e8ecd97660');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FurAPMisH1az"
      },
      "source": [
        "請注意，訓練DataFrame將有一個全為 -1 的標籤字段。這表示該特定數據集缺少標籤。在本教程中，我們將通過在主體和客體的屬性上編寫標籤函數來為訓練集分配概率標籤！"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2AFKUcGugpi"
      },
      "source": [
        "## 2. 編寫Labeling Functions (LFs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QX3fY8F_u7Ri"
      },
      "source": [
        "我們現在編寫標記函數來檢測邊界框對之間存在什麼關係。為此，我們可以將各種直覺編碼到標記函數中：\n",
        "\n",
        "- 分類直覺：關於這些關係中通常涉及的主詞與受詞類別的知識（例如，person通常是動詞ride和的主詞carry）\n",
        "- 空間直覺：關於主詞與動詞的相對位置的知識（例如，主詞通常高於動詞的受詞ride）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IjxWGy6H1a2"
      },
      "source": [
        "RIDE = 0\n",
        "CARRY = 1\n",
        "OTHER = 2\n",
        "ABSTAIN = -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zyph8mXpvDzP"
      },
      "source": [
        "我們從編碼分類直覺的標記函數開始：我們使用關於共同的主題-客體類別對的知識RIDE，CARRY以及關於哪些主題或客體不太可能涉及這兩種關係的知識。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQHD7_RhH1a5"
      },
      "source": [
        "from snorkel.labeling import labeling_function\n",
        "\n",
        "# Category-based LFs\n",
        "@labeling_function()\n",
        "def lf_ride_object(x):\n",
        "    if x.subject_category == \"person\":\n",
        "        if x.object_category in [\n",
        "            \"bike\",\n",
        "            \"snowboard\",\n",
        "            \"motorcycle\",\n",
        "            \"horse\",\n",
        "            \"bus\",\n",
        "            \"truck\",\n",
        "            \"elephant\",\n",
        "        ]:\n",
        "            return RIDE\n",
        "    return ABSTAIN\n",
        "\n",
        "\n",
        "@labeling_function()\n",
        "def lf_carry_object(x):\n",
        "    if x.subject_category == \"person\":\n",
        "        if x.object_category in [\"bag\", \"surfboard\", \"skis\"]:\n",
        "            return CARRY\n",
        "    return ABSTAIN\n",
        "\n",
        "\n",
        "@labeling_function()\n",
        "def lf_carry_subject(x):\n",
        "    if x.object_category == \"person\":\n",
        "        if x.subject_category in [\"chair\", \"bike\", \"snowboard\", \"motorcycle\", \"horse\"]:\n",
        "            return CARRY\n",
        "    return ABSTAIN\n",
        "\n",
        "\n",
        "@labeling_function()\n",
        "def lf_not_person(x):\n",
        "    if x.subject_category != \"person\":\n",
        "        return OTHER\n",
        "    return ABSTAIN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqMh6n0tvPkl"
      },
      "source": [
        "現在編碼空間直覺，其中包括測量邊界框之間的距離並比較它們的相對區域。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiLkPdTBH1a7"
      },
      "source": [
        "YMIN = 0\n",
        "YMAX = 1\n",
        "XMIN = 2\n",
        "XMAX = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIVtiWzJH1bC"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Distance-based LFs\n",
        "@labeling_function()\n",
        "def lf_ydist(x):\n",
        "    if x.subject_bbox[XMAX] < x.object_bbox[XMAX]:\n",
        "        return OTHER\n",
        "    return ABSTAIN\n",
        "\n",
        "\n",
        "@labeling_function()\n",
        "def lf_dist(x):\n",
        "    if np.linalg.norm(np.array(x.subject_bbox) - np.array(x.object_bbox)) <= 1000:\n",
        "        return OTHER\n",
        "    return ABSTAIN\n",
        "\n",
        "\n",
        "def area(bbox):\n",
        "    return (bbox[YMAX] - bbox[YMIN]) * (bbox[XMAX] - bbox[XMIN])\n",
        "\n",
        "\n",
        "# Size-based LF\n",
        "@labeling_function()\n",
        "def lf_area(x):\n",
        "    if area(x.subject_bbox) / area(x.object_bbox) <= 0.5:\n",
        "        return OTHER\n",
        "    return ABSTAIN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwapyteWvZg5"
      },
      "source": [
        "標記函數具有不同的經驗準確性和覆蓋範圍。由於我們選擇的關係中的類別不平衡，標記OTHER類的標記函數比RIDE或CARRY的標記函數具有更高的覆蓋率。這也反映了數據集中類的分佈。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "md-exclude-output"
        ],
        "id": "eaSNKjq_H1bD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b616ce63-79e3-41bf-c11b-32ce9ec9a35e"
      },
      "source": [
        "from snorkel.labeling import PandasLFApplier\n",
        "\n",
        "lfs = [\n",
        "    lf_ride_object,\n",
        "    lf_carry_object,\n",
        "    lf_carry_subject,\n",
        "    lf_not_person,\n",
        "    lf_ydist,\n",
        "    lf_dist,\n",
        "    lf_area,\n",
        "]\n",
        "\n",
        "applier = PandasLFApplier(lfs)\n",
        "L_train = applier.apply(df_train)\n",
        "L_valid = applier.apply(df_valid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26/26 [00:00<00:00, 3285.59it/s]\n",
            "100%|██████████| 26/26 [00:00<00:00, 5469.55it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjuXvnPiH1bE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "outputId": "18e9b771-6384-43c2-baa5-4f9a2e27fe5d"
      },
      "source": [
        "from snorkel.labeling import LFAnalysis\n",
        "\n",
        "Y_valid = df_valid.label.values\n",
        "LFAnalysis(L_valid, lfs).lf_summary(Y_valid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:72: FutureWarning: Pass labels=[-1  0  1  2] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
            "  \"will result in an error\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:72: FutureWarning: Pass labels=[-1  0  1  2] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
            "  \"will result in an error\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:72: FutureWarning: Pass labels=[-1  0  1  2] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
            "  \"will result in an error\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:72: FutureWarning: Pass labels=[-1  0  1  2] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
            "  \"will result in an error\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:72: FutureWarning: Pass labels=[-1  0  1  2] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
            "  \"will result in an error\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:72: FutureWarning: Pass labels=[-1  0  1  2] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
            "  \"will result in an error\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:72: FutureWarning: Pass labels=[-1  0  1  2] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
            "  \"will result in an error\", FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  j Polarity  Coverage  Overlaps  Conflicts  Correct  \\\n",
              "lf_ride_object    0      [0]  0.230769  0.230769   0.230769        5   \n",
              "lf_carry_object   1      [1]  0.076923  0.076923   0.076923        2   \n",
              "lf_carry_subject  2      [1]  0.038462  0.038462   0.038462        1   \n",
              "lf_not_person     3      [2]  0.307692  0.307692   0.038462        5   \n",
              "lf_ydist          4      [2]  0.576923  0.576923   0.307692        7   \n",
              "lf_dist           5      [2]  1.000000  0.846154   0.346154       13   \n",
              "lf_area           6      [2]  0.346154  0.346154   0.153846        5   \n",
              "\n",
              "                  Incorrect  Emp. Acc.  \n",
              "lf_ride_object            1   0.833333  \n",
              "lf_carry_object           0   1.000000  \n",
              "lf_carry_subject          0   1.000000  \n",
              "lf_not_person             3   0.625000  \n",
              "lf_ydist                  8   0.466667  \n",
              "lf_dist                  13   0.500000  \n",
              "lf_area                   4   0.555556  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-72cfa1f4-3b75-42cd-b3aa-96b74b12880d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>j</th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Coverage</th>\n",
              "      <th>Overlaps</th>\n",
              "      <th>Conflicts</th>\n",
              "      <th>Correct</th>\n",
              "      <th>Incorrect</th>\n",
              "      <th>Emp. Acc.</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>lf_ride_object</th>\n",
              "      <td>0</td>\n",
              "      <td>[0]</td>\n",
              "      <td>0.230769</td>\n",
              "      <td>0.230769</td>\n",
              "      <td>0.230769</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0.833333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lf_carry_object</th>\n",
              "      <td>1</td>\n",
              "      <td>[1]</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lf_carry_subject</th>\n",
              "      <td>2</td>\n",
              "      <td>[1]</td>\n",
              "      <td>0.038462</td>\n",
              "      <td>0.038462</td>\n",
              "      <td>0.038462</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lf_not_person</th>\n",
              "      <td>3</td>\n",
              "      <td>[2]</td>\n",
              "      <td>0.307692</td>\n",
              "      <td>0.307692</td>\n",
              "      <td>0.038462</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>0.625000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lf_ydist</th>\n",
              "      <td>4</td>\n",
              "      <td>[2]</td>\n",
              "      <td>0.576923</td>\n",
              "      <td>0.576923</td>\n",
              "      <td>0.307692</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>0.466667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lf_dist</th>\n",
              "      <td>5</td>\n",
              "      <td>[2]</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.846154</td>\n",
              "      <td>0.346154</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lf_area</th>\n",
              "      <td>6</td>\n",
              "      <td>[2]</td>\n",
              "      <td>0.346154</td>\n",
              "      <td>0.346154</td>\n",
              "      <td>0.153846</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>0.555556</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-72cfa1f4-3b75-42cd-b3aa-96b74b12880d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-72cfa1f4-3b75-42cd-b3aa-96b74b12880d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-72cfa1f4-3b75-42cd-b3aa-96b74b12880d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klcZJSydvuk_"
      },
      "source": [
        "## 3. 訓練標籤模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFiadUZ5v2W-"
      },
      "source": [
        "訓練`LabelModel`來為未標記的訓練集分配訓練標籤。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCLUsQqaH1bE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "680f3b66-e404-408b-c4ab-5ab597f5c002"
      },
      "source": [
        "from snorkel.labeling.model import LabelModel\n",
        "\n",
        "label_model = LabelModel(cardinality=3, verbose=True)\n",
        "label_model.fit(\n",
        "    L_train, \n",
        "    seed=123, \n",
        "    lr=0.01, \n",
        "    log_freq=10, \n",
        "    n_epochs=100\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:Computing O...\n",
            "INFO:root:Estimating \\mu...\n",
            "  0%|          | 0/100 [00:00<?, ?epoch/s]INFO:root:[0 epochs]: TRAIN:[loss=1.612]\n",
            "  1%|          | 1/100 [00:00<00:10,  9.21epoch/s]INFO:root:[10 epochs]: TRAIN:[loss=0.474]\n",
            "INFO:root:[20 epochs]: TRAIN:[loss=0.248]\n",
            "INFO:root:[30 epochs]: TRAIN:[loss=0.092]\n",
            "INFO:root:[40 epochs]: TRAIN:[loss=0.084]\n",
            "INFO:root:[50 epochs]: TRAIN:[loss=0.062]\n",
            "INFO:root:[60 epochs]: TRAIN:[loss=0.049]\n",
            "INFO:root:[70 epochs]: TRAIN:[loss=0.042]\n",
            "INFO:root:[80 epochs]: TRAIN:[loss=0.033]\n",
            "INFO:root:[90 epochs]: TRAIN:[loss=0.026]\n",
            "100%|██████████| 100/100 [00:00<00:00, 485.10epoch/s]\n",
            "INFO:root:Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUR0WrZ2xAzd"
      },
      "source": [
        "使用F1衡量模型\n",
        "`F1 = 2 * (precision * recall) / (precision + recall)`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbi_GaLKH1bF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5368c305-adbb-468c-df0b-a6741e8ed078"
      },
      "source": [
        "label_model.score(L_valid, Y_valid, metrics=[\"f1_micro\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Metrics calculated over data points with non-abstain labels only\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'f1_micro': 0.5769230769230769}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8pc2bmbxR3h"
      },
      "source": [
        "## 4. 訓練分類器"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POBs4cW_xcAt"
      },
      "source": [
        "現在，您可以使用這些訓練標籤來訓練任何標準判別模型，例如現成的 [ResNet](https://github.com/KaimingHe/deep-residual-networks)，它應該學會在我們開發的 LF 之外進行泛化！"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8xkOLQHH1bG"
      },
      "source": [
        "#### Create DataLoaders for Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-NzzNbcH1bG"
      },
      "source": [
        "from snorkel.classification import DictDataLoader\n",
        "from model import SceneGraphDataset, create_model\n",
        "\n",
        "df_train[\"labels\"] = label_model.predict(L_train)\n",
        "\n",
        "if sample:\n",
        "    TRAIN_DIR = \"data/VRD/sg_dataset/samples\"\n",
        "else:\n",
        "    TRAIN_DIR = \"data/VRD/sg_dataset/sg_train_images\"\n",
        "\n",
        "dl_train = DictDataLoader(\n",
        "    SceneGraphDataset(\"train_dataset\", \"train\", TRAIN_DIR, df_train),\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "dl_valid = DictDataLoader(\n",
        "    SceneGraphDataset(\"valid_dataset\", \"valid\", TRAIN_DIR, df_valid),\n",
        "    batch_size=16,\n",
        "    shuffle=False,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNqTVA8FH1bH"
      },
      "source": [
        "#### 定義模型架構"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yvz23jSH1bH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "29fc13409f38487a8b05ef4255ecbac6",
            "769e9a17b88d47fda1aee2a3ed739190",
            "31f90659a5eb42adb7c322a0b8578ca6",
            "59c2834a97aa4a888e6cc741b4028319",
            "4a65825500944be48d40d96a869d85b2",
            "52facabac00c49ceb92633884c1957c2",
            "9552000d9ac54672b127d04bdad9ffa0",
            "2ee2e86fd90e4cd88d661532b6664dbd",
            "3cc37e65ec69421c8b10f5ed0a86b9b2",
            "9b9f1136c5b24d949c7e796d549979cb",
            "7ae2c7366ecb4162a314b11dd6f1808f"
          ]
        },
        "outputId": "f0140ecb-ebc9-403b-e73a-0969652d5713"
      },
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "# initialize pretrained feature extractor\n",
        "cnn = models.resnet18(pretrained=True)\n",
        "model = create_model(cnn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "29fc13409f38487a8b05ef4255ecbac6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:Created task: visual_relation_task\n",
            "INFO:root:No cuda device available. Switch to cpu instead.\n",
            "INFO:root:Created multi-task model MultitaskClassifier that contains task(s) {'visual_relation_task'} from 6 operations (0 shared) and 4 modules (0 shared).\n",
            "INFO:root:No cuda device available. Switch to cpu instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzgLvaHIH1bH"
      },
      "source": [
        "### 訓練與評估模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "md-exclude-output"
        ],
        "id": "jO0WKO-ZH1bH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ded1c5d8-f994-447a-ec23-45bf8e61ca9b"
      },
      "source": [
        "from snorkel.classification import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    n_epochs=1,  # increase for improved performance\n",
        "    lr=1e-3,\n",
        "    checkpointing=True,\n",
        "    checkpointer_config={\"checkpoint_dir\": \"checkpoint\"},\n",
        ")\n",
        "trainer.fit(model, [dl_train])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:Save checkpoints at 'checkpoint' every 1.0 epochs.\n",
            "INFO:root:Evaluating every 1.0 epochs.\n",
            "INFO:root:Using optimizer Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0.0\n",
            ")\n",
            "INFO:root:Start training...\n",
            "Epoch 0::  50%|█████     | 1/2 [00:04<00:04,  4.31s/it, model/all/train/loss=1.83, model/all/train/lr=0.001]INFO:root:checkpoint_runway condition has been met. Start checkpointing.\n",
            "INFO:root:[MultitaskClassifier] Model saved in checkpoint/checkpoint_1.0.pth\n",
            "INFO:root:Save checkpoint at 1.0 epochs at checkpoint/checkpoint_1.0.pth.\n",
            "INFO:root:Save best model of metric model/all/train/loss at checkpoint/best_model_model_all_train_loss.pth\n",
            "Epoch 0:: 100%|██████████| 2/2 [00:07<00:00,  3.55s/it, model/all/train/loss=1.72, model/all/train/lr=0.001]\n",
            "INFO:root:Clear all checkpoints other than best so far.\n",
            "INFO:root:Loading the best model from checkpoint/best_model_model_all_train_loss.pth.\n",
            "INFO:root:[MultitaskClassifier] Model loaded from checkpoint/best_model_model_all_train_loss.pth\n",
            "INFO:root:No cuda device available. Switch to cpu instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N88EpcPmH1bI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2300c0be-1e15-46dc-e3af-833a1bf63429"
      },
      "source": [
        "model.score([dl_valid])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'visual_relation_task/valid_dataset/valid/f1_micro': 0.34615384615384615}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNpnAqufyNWJ"
      },
      "source": [
        "- 我們已經成功訓練了一個視覺關係檢測模型！使用關於視覺關係中的對像如何相互作用的分類和空間直覺，我們能夠在多類分類設置中為 VRD 數據集中的對像對分配高質量的訓練標籤。\n",
        "\n",
        "- 有關 Snorkel 如何用於視覺關係任務的更多信息，請參閱[ICCV 2019 論文](https://arxiv.org/abs/1904.11622)！"
      ]
    }
  ]
}